# Word embeddings.
glove_300d {
  path = PubMed-shuffle-win-2.txt
  size = 200
}
glove_300d_filtered {
  path = PubMed-shuffle-win-2.txt.filtered
  size = 200
}

glove_300d_filtered_KB {
  path = PubMed-shuffle-win-2.txt.filtered_KB
  size = 200
}

glove_300d_2w {
  path = PubMed-shuffle-win-2.txt
  size = 200
}

# Distributed training configurations.
two_local_gpus {
  addresses {
    ps = [localhost:2222]
    worker = [localhost:2223, localhost:2224]
  }
  gpus = [0, 1]
}

# Main configuration.
best {
  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 50
  top_span_ratio = 0.7

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "char_vocab.txt"
  context_embeddings = ${glove_300d_filtered}
  KB_embeddings = ${glove_300d_filtered_KB}  #kkkkkkkkkk
  head_embeddings = ${glove_300d_2w}
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30
  bert_size=768
  
  use_metadata = true
  use_features = true
  model_heads = true
  
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  top_kb_ratio = 0.5
  
        
  kbmaxnum=5
  omcs_k = 150
  organisms_k = 10
  proteins_k = 10
  mygenes_k =10
  genes_k =10
  interaction_types_k =10
  
  
  #coarsekb
  coarsekomcs=30
  

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.0001
  decay_rate = 0.999
  decay_frequency = 100
  
  #KB
  noKB=true
  average_KB=false
  attention_KB=false

  # Other.
  train_path = train5withkb.conll.jsonlines
  eval_path = dev5withkb.conll.jsonlines
  conll_eval_path = dev4.conll
  lm_path = elmo_pmid.hdf5   
  bert_path=bert_pmid.hdf5
  
  eval_frequency = 5000
  report_frequency = 100
  log_root = logs
  cluster = ${two_local_gpus}
}

average = ${best} {
  log_root = logs_kb
  noKB=false
  average_KB=true
  attention_KB=false
}

att= ${best} {
  log_root = logs_kb
  noKB=false
  average_KB=false
  attention_KB=true
}

# For evaluation. Do not use for training (i.e. only for predict.py, evaluate.py, and demo.py). Rename `best` directory to `final`.
final = ${best} {
  context_embeddings = ${glove_300d}
  head_embeddings = ${glove_300d_2w}
  #lm_path =test_elmo_pmid.hdf5
  eval_path = test.english.new2.jsonlines
  conll_eval_path = test.english.v4_gold_conll
  bert_path=bert_test.hdf5
}
finalave = ${average} {
  context_embeddings = ${glove_300d}
  head_embeddings = ${glove_300d_2w}
  lm_path = test_elmo_pmid.hdf5
  eval_path = test5withkb.conll.jsonlines
  conll_eval_path = test.english.v4_gold_conll
  bert_path=bert_test.hdf5
}
finalatt = ${att} {
  context_embeddings = ${glove_300d}
  head_embeddings = ${glove_300d_2w}
  lm_path = test_elmo_pmid.hdf5
  eval_path = test5withkb.conll.jsonlines
  conll_eval_path = test.english.v4_gold_conll
  bert_path=bert_test.hdf5
}

# Baselines.
c2f_100_ant = ${best} {
  max_top_antecedents = 100
}
c2f_250_ant = ${best} {
  max_top_antecedents = 250
}
c2f_1_layer = ${best} {
  coref_depth = 1
}
c2f_3_layer = ${best} {
  coref_depth = 3
}
distance_50_ant = ${best} {
  max_top_antecedents = 50
  coarse_to_fine = false
  coref_depth = 1
}
distance_100_ant = ${distance_50_ant} {
  max_top_antecedents = 100
}
distance_250_ant = ${distance_50_ant} {
  max_top_antecedents = 250
}
